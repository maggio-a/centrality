% !TeX root = p2p.tex

\section{Approximation of centrality indices}
\label{sec:mbfs}

The high cost of \deccen{} justifies the investigation of approximation techniques in order to reduce communication and memory overhead at the expense of numerical accuracy.

%The main issues with \deccen{} are the high cost in terms of number of messages exchanged and the local requirement of a quadratic space data structure, which make its use impractical in networks of reasonable size.

Eppstein and Wang introduced in \cite{ew2004} an approximation algorithm for the computation of closeness centrality. Their approach is to sample the contributions of a subset of nodes to any closeness value, and extrapolate from those an approximated measure; the contribution of a node $s$ to the closeness of $v$ is taken to be the distance $d(v,s)$ and is computed by solving a Single-Source-Shortest-Path problem with $s$ as source. Brandes in \cite{brandes2007} extended this scheme to the approximation of betweenness centrality: his method relies on reformulating the betweenness in terms of contributions from other nodes and employs an augmented SSSP to compute those contributions recursively, as he originally proposed in \cite{brandes2001}.
This approach can be applied in a decentralized way (by adapting the SSSP problem, which in th undirected case becomes a decentralized Breadth-First Search) and extended to the estimation of stress centrality, leading to the definition of another decentralized algorithm for the computation (and estimation) of centrality indices.

%Eppstein and Wang introduced in \cite{ew2004} an approximation algorithm for the computation of closeness centrality. Their approach is to sample from a subset of nodes the contributions to the closeness of all the nodes in the network, and extrapolate from those an approximated measure; the contribution of a node $s$ to any $v$ is taken to be the distance $d(v,s)$ and is computed by solving a Single-Source-Shortest-Path problem with $s$ as source. Brandes in \cite{brandes2007} extended this scheme to the approximation of betweenness centrality: his method relies on reformulating the betweenness in terms of contributions from other nodes and employs an augmented SSSP to compute those contributions recursively, as he originally proposed in \cite{brandes2001}. This approach can be applied in a decentralized way (by adapting the SSSP problem, which in this case becomes a decentralized Breadth-First Search) and extended to the estimation of stress centrality, leading to the definition of another decentralized algorithm for the computation (and estimation) of centrality indices.

\subsection{Reformulating centrality indices}

%The first step is to write each centrality index of a node $v$ as a sum of terms where each term denotes the contribution of a second node $s$ (referred to as the \emph{source} of the contribution) to the index value.

The first step is to rewrite each centrality index as a sum of terms where each term denotes the contribution of a different \emph{source} node.

\paragraph{Contribution of a source to Closeness centrality}
As already mentioned, the contribution of a source node $s$ to the closeness centrality of $v$ is the distance $d(s,v)$. The closeness of a node is simply the sum of the contributions of all the other nodes to its centrality divided by $n-1$.

\paragraph{Contribution of a source to Betweenness centrality}
The contribution of a source $s$ to the betweenness centrality of $v$ is the \emph{dependency} of $s$ on $v$ introduced in \cite{brandes2001}:
\begin{equation}
\delta(s|v) = \sum_{t \in V} \frac{\sigma_{st}(v)}{\sigma_{st}},
\end{equation}
that allows to rewrite the betweenness centrality of $v$ as
\begin{equation}
BC(v) = \sum_{s \in V} \delta(s|v).
\end{equation}

\paragraph{Contribution of a source to Stress centrality}
The contribution to stress centrality is analogous to the contribution to betweenness centrality:
\begin{equation} \label{eq:contrib:sc}
\sigma(s|v) = \sum_{t \in V} \sigma_{st}(v),
\end{equation}
and the stress centrality of $v$ is rewritten as
\begin{equation}
SC(v) = \sum_{s \in V} \sigma(s|v).
\end{equation}

\subsection{Computing contributions from a single source}
\label{sec:recursive}

A decentralized Breadth First Search can be adapted to compute the contribution of a source to any of the three centrality indices considered.

The closeness centrality contribution is simply the distance from the source to the node and it can be computed directly during the visit.

For betweenness centrality, \citet{brandes2001} has shown that dependencies of a source obey a recursive relation expressed in terms of predecessors set:
\begin{theorem}[Brandes, 2001]
\label{th:contrib:bc}
The dependency of $s \in V$ on any $v \in V$ obeys
\begin{equation} \label{eq:th:contrib:bc}
\delta(s|v) = \sum_{w : v \in P_s(w)} \frac{\sigma_{sv}}{\sigma_{sw}} \cdot (1 + \delta(s|w)) .
\end{equation}
\end{theorem}
For stress centrality contributions a similar relation holds (the proof is reported in the appendix):
\begin{theorem}
\label{th:contrib:sc}
The stress centrality contribution of $s \in V$ on any $v \in V$ obeys
\begin{equation} \label{eq:th:contrib:sc}
\sigma(s|v) = \sum_{w : v \in P_s(w)} \sigma_{sv} \cdot \left( 1 + \frac{\sigma(s|w)}{\sigma_{sw}} \right) .
\end{equation}
\end{theorem}

The predecessor sets can be easily discovered by the BFS algorithm during the network exploration, while contributions are computed with a backward walk from the frontier of the BF-Tree to the source of the visit.

\subsection{Random sampling of source nodes}

To let the algorithm operate in a decentralized way, each node independently initiates a visit with a given probability $p$, which reflects the fraction of the network sampled. Even if the number $k$ of samples is not known beforehand, all the nodes will be able to compute it by counting the number of visits in which they will take part.

The result of sampling from a source node $v_i$ at a node $u$ is modeled with the following random variables (that rely on the contributions of $v_i$ to $u$)
\begin{equation*}
X_i(u) = \frac{n}{n-1} \cdot d(v_i,u) , \qquad
Y_i(u) = n \cdot \delta(v_i|u) , \qquad
Z_i(u) = n \cdot \sigma(v_i|u) ,
\end{equation*}
which are then used to define the estimators of the three centrality indices as
\begin{equation*}
\widetilde{C}_C(u) = \sum_{i=1}^k \frac{X_i(u)}{k}, \qquad
\widetilde{C}_B(u) = \sum_{i=1}^k \frac{Y_i(u)}{k}, \qquad
\widetilde{C}_S(u) = \sum_{i=1}^k \frac{Z_i(u)}{k} .
\end{equation*}

The proof of the next theorem -- which ensures that the centrality estimators are unbiased -- is reported in the appendix.

\begin{theorem}[Unbiased estimators]
\label{th:expect}
The expected values of the centrality estimators are the actual centrality indices:
 \begin{enumerate}[label=\textup{(\alph*)}]
  \item $\mathbf{E}[\widetilde{C}_C(u)] = C_C(u)$,
  \item $\mathbf{E}[\widetilde{C}_S(u)] = C_S(u)$,
  \item $\mathbf{E}[\widetilde{C}_B(u)] = C_B(u)$.
  \end{enumerate}
\end{theorem}

\subsection{\multibfs{} algorithm specification}

The only parameter of the algorithm is the probability $p$ of a node becoming the source of a decentralized BFS and having its centrality contributions sampled at every location. The network size $n$ is assumed to be known in advance to all the nodes, otherwise it can be easily computed during the backtracking phase of the augmented BFS and broadcast by one or more sources.

\pagebreak
Note that if this algorithm is executed with $p=1$ it is basically the decentralized version of the algorithm described in \cite{brandes2001}, and at the end of the execution the estimators will yield the exact centrality values.

\subsubsection{Message types}

The messages used during the algorithm execution are the following:

\begin{description}[leftmargin=0cm]
\item[\mdiscargs{s}{u}{d}] These messages are used during the visit from a source to build the predecessors sets at each node. The fields are the source $s$ of the visit, the sender $u$, the distance $d$ of the sender to the source (that is, $d = d(s,u)$) and the number of shortest path from the source to the sender $\sigma_{su}$.

\item[\mrepargs{s}{v}] These messages are sent by a node $v$ as part of the backward walk to inform its predecessors of the computed contributions and to allow them to compute their own by applying the recursive relations introduced in section \ref{sec:recursive}.
\end{description}

\subsubsection{Visit states}

Visit states are parametric with respect to the discovery from a source $s \in V$. A node $v$ is in state:
\begin{description}[leftmargin=0cm]
\item[\swait{s}] if it has not yet received any \mdisc{} having $s$ as source.
\item[\sact{s}] if it has received one or more \mdisc{} messages with source $s$ and has not yet computed the contributions of $s$ to its centrality indices.
\item[\scomp{s}] if it has computed the contributions of $s$ to its centrality indices and reported them back to each predecessor in $P_s(v)$.
\end{description}

\subsubsection{Node state}
Each node $v$ maintains three centrality accumulators $C_C$, $C_B$ and $C_S$ like in \deccen{}, and a counter $k$ to track the number of sample nodes involved in the procedure. Furthermore, while a node is in state \sact{s} it will need to partition the set of neighbors $N_v$ in three subsets: the set of predecessors $P_s(v)$, the set of siblings $S_s(v)$ and the set of children $C_s(v)$, and to track the contributions of $s$ to its centrality values with three parametric accumulators $C_C^{(s)}$, $C_B^{(s)}$ and $C_S^{(s)}$.

\subsubsection{Protocol initialization}
Upon initialization, a node $v$ clears its centrality accumulators and the counter $k$, and enters state \swait{s} for all $s \in V$. Then, during the initial step it initiates a visit with probability $p$ by entering state \sact{v}, setting $P_v(v) = \emptyset$ and sending a \mdiscargsfull{v}{v}{0}{1} to every neighbor.

\subsubsection{Step actions}
\label{multibfs:step}
The actions performed by a node $v$ at each step are:

\begin{algosteps}
  \item The messages received at the current step are divided by type and then \mdisc{} messages are grouped by source.
  \item For each group of \mdiscargs{s}{u}{d} messages having $s$ as a common source (at distance $d$), with $v$ in state \swait{s}:
  \begin{algosteps}
    \item Change state to \sact{s} and initialize $C_C^{(s)} \gets d+1$, $C_B^{(s)} \gets 0$ and $C_S^{(s)} \gets 0$; then collect all the senders $u$ in the predecessor set $P_s(v)$ and compute $\sigma_{sv} = \sum_{u \in P_s(v)} \sigma_{su}$.
    \item If $P_s(v) = N_v$ send a \mrepleaf{s}{v} message to all the predecessors $u \in P_s(v)$ and change state to \scomp{s}. 
    \item Otherwise, send to all $w \in N_v \setminus P_s(v)$ a \mdiscargs{s}{v}{d+1} message and change state to \sact{s}.
  \end{algosteps}
    \item For each group of \mdiscargs{s}{u}{d} messages having source $s$ and distance $d$ with $v$ in state \sact{s}:
    \begin{algosteps}
      \item Under the synchronous model assumption $v$ can only receive such messages from nodes $u$ such that $d(s,u) = d(s,v)$. Collect these nodes in the set $S_s(v)$ of the siblings of $v$ with respect to $s$. (Note that these messages are received exactly one step after $v$ has been contacted by its predecessors).
    \end{algosteps}
  \item For each \mrepargs{s}{w} message received with $v$ in state \sact{s}:
  \begin{algosteps}
      \item Add $w$ to the children set $C_s(v)$.
      \item Update $C_B^{(s)} \gets C_B^{(s)} + \frac{\sigma_{sv}}{\sigma_{sw}} \cdot (1 + \delta(s|w))$.
      \item Update $C_S^{(s)} \gets C_S^{(s)} + \sigma_{sw} \cdot ( 1 + \frac{\sigma(s|w)}{\sigma_{sw}})$.
    \end{algosteps}
  \item For any $s \in V$ with $v$ is in state \sact{s}, if $P_s(v) \cup S_s(v) \cup C_s(v) = N_v$ then:
  \begin{algosteps}
      \item If $s \neq v$ update the accumulators:
      \begin{itemize}
        \item[-] $C_C \gets C_C + C_C^{(s)}$
        \item[-] $C_B \gets C_B + C_B^{(s)}$
        \item[-] $C_S \gets C_S + C_S^{(s)}$
      \end{itemize}
      and send a \mrepargs{s}{v} message to all the predecessors contained in $P_s(v)$.
      \item Increment the counter $k$ and change state to \scomp{s}.
    \end{algosteps}
\end{algosteps}
A node $v$ can obtain the value of the estimators in the following way:
\begin{equation*}
\widetilde{C}_C(v) = \frac{n C_C}{k(n-1)} , \qquad
\widetilde{C}_B(v) = \frac{n C_B}{k} , \qquad
\widetilde{C}_S(v) = \frac{n C_S}{k} .
\end{equation*}

\subsection{Cost analysis}

Each independent visit requires $O(m)$ messages for the \mdisc{} phase and $O(m)$ messages to backtrack with \mrep{} messages. The parameter $p$ determines the fraction of nodes $pn$ that initiate a \mdisc{} search, so the total number of messages is $O(\lceil pn \rceil m)$.

In terms of memory consumption, note that for each $v \in V$ there are at most $\lceil pn \rceil$ other nodes $s$ for which $v$ is in state \sact{s} and needs to partition its neighbor set $N_v$ in the three subsets $P_s(v)$, $S_s(v)$ and $C_s(v)$, so the cost is $O(\lceil pn \rceil N_v)$.